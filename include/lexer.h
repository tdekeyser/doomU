/**
* A tokenizer cuts a sequence of inputs into a series of statments/words
* that are labeled.
*/
#ifndef DOOMU_TOKENIZER_H
#define DOOMU_TOKENIZER_H

#include "function.h"


Stream *lex(const char *filename);


#endif /* DOOMU_TOKENIZER_H */